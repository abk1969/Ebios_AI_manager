"""
üéº ORCHESTRATEUR WORKSHOP 1 AVANC√â
Orchestration IA utilisant LangChain + Instructor + A2A
ATTENTION: Service additif qui ne casse pas l'existant
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
import json

# Imports conditionnels pour √©viter les erreurs si librairies manquantes
try:
    from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
    from langchain.callbacks.base import BaseCallbackHandler
    from langchain.agents import AgentExecutor, create_openai_functions_agent
    from langchain.tools import BaseTool
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    logging.warning("üîß LangChain non disponible, mode simulation activ√©")

try:
    import instructor
    INSTRUCTOR_AVAILABLE = True
except ImportError:
    INSTRUCTOR_AVAILABLE = False
    logging.warning("üîß Instructor non disponible, mode simulation activ√©")

try:
    from pydantic import BaseModel, Field
    PYDANTIC_AVAILABLE = True
except ImportError:
    PYDANTIC_AVAILABLE = False
    logging.warning("üîß Pydantic non disponible, mode simulation activ√©")

    # Classe de fallback
    class BaseModel:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)

        def __dict__(self):
            return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}

    def Field(**kwargs):
        return None

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    logging.warning("üîß Redis non disponible, mode m√©moire locale activ√©")

# Import des services existants (sans les casser)
try:
    from .workshop1_ai_service import Workshop1AIService
    from .suggestion_engine import SuggestionEngine
    EXISTING_SERVICES_AVAILABLE = True
except ImportError:
    EXISTING_SERVICES_AVAILABLE = False
    logging.warning("üîß Services existants non trouv√©s, mode autonome activ√©")

# Import des nouveaux services IA avanc√©s
try:
    from .semantic_analyzer import SemanticAnalyzerFactory
    from .ml_suggestion_engine import MLSuggestionEngineFactory
    ADVANCED_AI_SERVICES_AVAILABLE = True
except ImportError:
    ADVANCED_AI_SERVICES_AVAILABLE = False
    logging.warning("üîß Services IA avanc√©s non trouv√©s, mode basique activ√©")

# Import des services RAG et traitement de documents
try:
    from .ebios_rag_service import EbiosRAGServiceFactory
    from .document_processor import DocumentProcessorFactory
    RAG_SERVICES_AVAILABLE = True
except ImportError:
    RAG_SERVICES_AVAILABLE = False
    logging.warning("üîß Services RAG non trouv√©s, mode sans RAG activ√©")

logger = logging.getLogger(__name__)

# === MOD√àLES PYDANTIC POUR INSTRUCTOR ===

class EbiosElement(BaseModel):
    """√âl√©ment EBIOS RM structur√©"""
    id: str
    name: str
    description: str
    category: str
    criticality: str = Field(description="Criticit√©: low, medium, high, critical")
    confidence: float = Field(ge=0, le=1, description="Confiance de l'IA")
    suggestions: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class WorkshopAnalysisResult(BaseModel):
    """R√©sultat d'analyse Workshop 1"""
    mission_id: str
    completion_percentage: float = Field(ge=0, le=100)
    quality_score: float = Field(ge=0, le=100)
    coherence_score: float = Field(ge=0, le=100)
    ebios_compliance: float = Field(ge=0, le=100)
    elements: List[EbiosElement]
    suggestions: List[str]
    next_steps: List[str]
    analysis_timestamp: datetime = Field(default_factory=datetime.now)

class OrchestrationPlan(BaseModel):
    """Plan d'orchestration des agents"""
    plan_id: str
    mission_id: str
    agents_required: List[str]
    execution_order: List[str]
    estimated_duration: int  # en secondes
    fallback_strategy: str
    created_at: datetime = Field(default_factory=datetime.now)

# === OUTILS LANGCHAIN POUR EBIOS RM ===

class EbiosAnalysisTool(BaseTool):
    """Outil d'analyse EBIOS RM pour LangChain"""
    name = "ebios_analysis"
    description = "Analyse les √©l√©ments EBIOS RM et fournit des suggestions d'am√©lioration"
    
    def __init__(self, workshop_service: Optional[Any] = None):
        super().__init__()
        self.workshop_service = workshop_service
    
    def _run(self, elements: str) -> str:
        """Ex√©cute l'analyse EBIOS RM"""
        try:
            # Parse les √©l√©ments
            data = json.loads(elements)
            
            # Analyse basique si service non disponible
            if not self.workshop_service:
                return self._basic_analysis(data)
            
            # Utilise le service existant si disponible
            return self._advanced_analysis(data)
            
        except Exception as e:
            logger.error(f"Erreur analyse EBIOS: {e}")
            return f"Erreur d'analyse: {str(e)}"
    
    def _basic_analysis(self, data: Dict[str, Any]) -> str:
        """Analyse basique sans services externes"""
        business_values = data.get('business_values', [])
        essential_assets = data.get('essential_assets', [])
        
        analysis = {
            "completion": len(business_values) > 0 and len(essential_assets) > 0,
            "suggestions": [
                "Ajoutez plus de d√©tails aux descriptions",
                "V√©rifiez les liens entre valeurs m√©tier et biens essentiels",
                "Compl√©tez les crit√®res de s√©curit√©"
            ]
        }
        
        return json.dumps(analysis, ensure_ascii=False)
    
    def _advanced_analysis(self, data: Dict[str, Any]) -> str:
        """Analyse avanc√©e avec services existants"""
        # Ici on utiliserait le Workshop1AIService existant
        return json.dumps({"status": "advanced_analysis_ready"}, ensure_ascii=False)

class EbiosSuggestionTool(BaseTool):
    """Outil de suggestions EBIOS RM pour LangChain"""
    name = "ebios_suggestions"
    description = "G√©n√®re des suggestions contextuelles pour am√©liorer l'analyse EBIOS RM"
    
    def _run(self, context: str) -> str:
        """G√©n√®re des suggestions contextuelles"""
        try:
            context_data = json.loads(context)
            current_step = context_data.get('current_step', 'unknown')
            
            suggestions_map = {
                'business-values': [
                    "Identifiez vos processus m√©tier critiques",
                    "Pensez aux exigences r√©glementaires",
                    "Consid√©rez votre r√©putation et image de marque"
                ],
                'essential-assets': [
                    "Cartographiez vos informations sensibles",
                    "Identifiez vos processus cl√©s",
                    "Documentez votre savoir-faire critique"
                ],
                'supporting-assets': [
                    "Inventoriez vos syst√®mes techniques",
                    "Identifiez vos ressources humaines cl√©s",
                    "Cartographiez votre infrastructure"
                ]
            }
            
            suggestions = suggestions_map.get(current_step, ["Continuez votre analyse EBIOS RM"])
            
            return json.dumps({
                "suggestions": suggestions,
                "step": current_step,
                "confidence": 0.85
            }, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration suggestions: {e}")
            return json.dumps({"error": str(e)}, ensure_ascii=False)

# === ORCHESTRATEUR PRINCIPAL ===

class Workshop1Orchestrator:
    """
    Orchestrateur principal pour Workshop 1
    Int√®gre LangChain, Instructor et les services existants
    """
    
    def __init__(self):
        self.session_id = f"w1_orchestrator_{datetime.now().timestamp()}"
        self.memory_store = {}  # M√©moire locale par d√©faut
        self.existing_services = {}
        self.advanced_ai_services = {}  # Nouveaux services IA
        self.rag_services = {}  # Services RAG et documents
        self.langchain_agent = None
        self.instructor_client = None
        self.redis_client = None  # Initialisation par d√©faut

        # Initialisation s√©curis√©e
        self._initialize_safely()
    
    def _initialize_safely(self):
        """Initialisation s√©curis√©e sans casser l'existant"""
        logger.info(f"üéº Initialisation Orchestrateur Workshop 1: {self.session_id}")
        
        # 1. Initialiser les services existants si disponibles
        if EXISTING_SERVICES_AVAILABLE:
            try:
                self.existing_services['workshop1'] = Workshop1AIService()
                self.existing_services['suggestions'] = SuggestionEngine()
                logger.info("‚úÖ Services existants charg√©s")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Services existants non disponibles: {e}")

        # 1.5. Initialiser les nouveaux services IA avanc√©s
        if ADVANCED_AI_SERVICES_AVAILABLE:
            try:
                self.advanced_ai_services['semantic_analyzer'] = SemanticAnalyzerFactory.create()
                self.advanced_ai_services['ml_suggestion_engine'] = MLSuggestionEngineFactory.create()
                logger.info("‚úÖ Services IA avanc√©s charg√©s")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Services IA avanc√©s non disponibles: {e}")

        # 1.6. Initialiser les services RAG et traitement de documents (MODE SIMPLIFI√â)
        if RAG_SERVICES_AVAILABLE:
            try:
                self.rag_services['rag_service'] = EbiosRAGServiceFactory.create()
                self.rag_services['document_processor'] = DocumentProcessorFactory.create()

                # Construire l'index simple (pas de LlamaIndex)
                if self.rag_services['rag_service']:
                    asyncio.create_task(self.rag_services['rag_service'].build_vector_index())

                logger.info("‚úÖ Services RAG charg√©s (mode simplifi√©)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Services RAG non disponibles: {e}")
        
        # 2. Initialiser Redis si disponible
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(
                    host='localhost', 
                    port=6379, 
                    decode_responses=True,
                    socket_connect_timeout=1
                )
                # Test de connexion
                self.redis_client.ping()
                logger.info("‚úÖ Redis connect√© pour m√©moire persistante")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Redis non disponible, m√©moire locale: {e}")
                self.redis_client = None
        
        # 3. Initialiser LangChain si disponible
        if LANGCHAIN_AVAILABLE:
            try:
                self._setup_langchain_agent()
                logger.info("‚úÖ Agent LangChain initialis√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è LangChain non disponible: {e}")
        
        # 4. Initialiser Instructor si disponible
        if INSTRUCTOR_AVAILABLE:
            try:
                # Instructor sera utilis√© pour structurer les r√©ponses
                logger.info("‚úÖ Instructor disponible pour structuration")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Instructor non disponible: {e}")
    
    def _setup_langchain_agent(self):
        """Configure l'agent LangChain pour EBIOS RM"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        # Outils EBIOS RM
        tools = [
            EbiosAnalysisTool(self.existing_services.get('workshop1')),
            EbiosSuggestionTool()
        ]
        
        # Prompt syst√®me pour EBIOS RM
        system_prompt = """Tu es un expert EBIOS RM qui aide les utilisateurs √† r√©aliser l'Atelier 1.
        
        Ton r√¥le:
        - Analyser les √©l√©ments fournis (valeurs m√©tier, biens essentiels, etc.)
        - Fournir des suggestions d'am√©lioration
        - V√©rifier la conformit√© ANSSI
        - Guider l'utilisateur √©tape par √©tape
        
        Utilise les outils disponibles pour analyser et sugg√©rer des am√©liorations.
        Sois pr√©cis, constructif et p√©dagogique."""
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad")
            ])
            
            # Note: Dans un vrai d√©ploiement, on utiliserait un LLM r√©el
            # Ici on simule pour √©viter les d√©pendances externes
            logger.info("ü§ñ Agent LangChain configur√© (mode simulation)")
            
        except Exception as e:
            logger.error(f"Erreur configuration LangChain: {e}")
    
    async def orchestrate_workshop_analysis(
        self, 
        mission_id: str,
        workshop_data: Dict[str, Any],
        user_context: Optional[Dict[str, Any]] = None
    ) -> WorkshopAnalysisResult:
        """
        Orchestration compl√®te de l'analyse Workshop 1
        """
        logger.info(f"üéº Orchestration analyse Workshop 1: {mission_id}")
        
        try:
            # 1. R√©cup√©rer le contexte utilisateur
            context = await self._get_user_context(mission_id, user_context)
            
            # 2. Analyser avec les services existants si disponibles
            if self.existing_services.get('workshop1'):
                existing_analysis = await self._analyze_with_existing_services(
                    mission_id, workshop_data
                )
            else:
                existing_analysis = self._basic_analysis(workshop_data)

            # 2.5. Enrichir avec l'analyse s√©mantique avanc√©e
            if self.advanced_ai_services.get('semantic_analyzer'):
                semantic_analysis = await self._analyze_with_semantic_ai(
                    workshop_data, existing_analysis
                )
                existing_analysis.update(semantic_analysis)

            # 2.6. Enrichir avec les suggestions ML
            if self.advanced_ai_services.get('ml_suggestion_engine'):
                ml_analysis = await self._analyze_with_ml_engine(
                    workshop_data, context, existing_analysis
                )
                existing_analysis.update(ml_analysis)

            # 2.7. Enrichir avec RAG EBIOS RM
            if self.rag_services.get('rag_service'):
                rag_analysis = await self._analyze_with_rag_service(
                    workshop_data, context, existing_analysis
                )
                existing_analysis.update(rag_analysis)

            # 3. Enrichir avec LangChain si disponible
            if LANGCHAIN_AVAILABLE and self.langchain_agent:
                enhanced_analysis = await self._enhance_with_langchain(
                    existing_analysis, workshop_data, context
                )
            else:
                enhanced_analysis = existing_analysis
            
            # 4. Structurer avec Instructor si disponible
            if INSTRUCTOR_AVAILABLE:
                structured_result = self._structure_with_instructor(enhanced_analysis)
            else:
                structured_result = self._create_basic_result(enhanced_analysis, mission_id)
            
            # 5. Sauvegarder le contexte
            await self._save_context(mission_id, structured_result, context)
            
            logger.info(f"‚úÖ Orchestration termin√©e: {mission_id}")
            return structured_result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur orchestration: {e}")
            # Fallback s√©curis√©
            return self._create_fallback_result(mission_id, workshop_data)
    
    async def _get_user_context(
        self, 
        mission_id: str, 
        user_context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """R√©cup√®re le contexte utilisateur depuis la m√©moire"""
        context = user_context or {}
        
        # Essayer Redis d'abord
        if self.redis_client:
            try:
                stored_context = self.redis_client.get(f"context:{mission_id}")
                if stored_context:
                    context.update(json.loads(stored_context))
                    logger.info(f"üìö Contexte r√©cup√©r√© depuis Redis: {mission_id}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur lecture Redis: {e}")
        
        # Fallback m√©moire locale
        if mission_id in self.memory_store:
            context.update(self.memory_store[mission_id])
            logger.info(f"üìö Contexte r√©cup√©r√© depuis m√©moire locale: {mission_id}")
        
        return context
    
    async def _analyze_with_existing_services(
        self, 
        mission_id: str, 
        workshop_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Utilise les services existants pour l'analyse"""
        try:
            workshop_service = self.existing_services['workshop1']
            
            # Appel au service existant
            analysis = await workshop_service.analyze_workshop_context(
                mission_id=mission_id,
                business_values=workshop_data.get('business_values', []),
                essential_assets=workshop_data.get('essential_assets', []),
                supporting_assets=workshop_data.get('supporting_assets', []),
                dreaded_events=workshop_data.get('dreaded_events', []),
                current_step=workshop_data.get('current_step')
            )
            
            logger.info("‚úÖ Analyse avec services existants r√©ussie")
            return analysis.__dict__ if hasattr(analysis, '__dict__') else analysis
            
        except Exception as e:
            logger.error(f"‚ùå Erreur services existants: {e}")
            return self._basic_analysis(workshop_data)
    
    def _basic_analysis(self, workshop_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse basique de fallback"""
        business_values = workshop_data.get('business_values', [])
        essential_assets = workshop_data.get('essential_assets', [])
        supporting_assets = workshop_data.get('supporting_assets', [])
        dreaded_events = workshop_data.get('dreaded_events', [])
        
        completion_status = {
            "business_values": len(business_values) >= 1,
            "essential_assets": len(essential_assets) >= 1,
            "supporting_assets": len(supporting_assets) >= 1,
            "dreaded_events": len(dreaded_events) >= 1
        }
        
        completed_sections = sum(completion_status.values())
        completion_percentage = (completed_sections / 4) * 100
        
        return {
            "completion_status": completion_status,
            "completion_percentage": completion_percentage,
            "quality_metrics": {
                "completeness": completion_percentage,
                "coherence": 75.0,
                "detail_level": 60.0,
                "ebios_compliance": completion_percentage * 0.8
            },
            "suggestions": [
                "Compl√©tez les sections manquantes",
                "Enrichissez les descriptions",
                "V√©rifiez la coh√©rence des liens"
            ],
            "next_steps": [
                "D√©finir les valeurs m√©tier" if not completion_status["business_values"] else None,
                "Identifier les biens essentiels" if not completion_status["essential_assets"] else None,
                "Cataloguer les biens supports" if not completion_status["supporting_assets"] else None,
                "D√©finir les √©v√©nements redout√©s" if not completion_status["dreaded_events"] else None
            ]
        }
    
    async def _enhance_with_langchain(
        self, 
        analysis: Dict[str, Any], 
        workshop_data: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Enrichit l'analyse avec LangChain"""
        # Simulation d'enrichissement LangChain
        enhanced = analysis.copy()
        enhanced["langchain_enhanced"] = True
        enhanced["ai_confidence"] = 0.85
        enhanced["contextual_suggestions"] = [
            "Suggestion enrichie par LangChain",
            "Analyse contextuelle avanc√©e"
        ]
        
        logger.info("ü§ñ Analyse enrichie avec LangChain")
        return enhanced
    
    def _structure_with_instructor(self, analysis: Dict[str, Any]) -> WorkshopAnalysisResult:
        """Structure le r√©sultat avec Instructor"""
        try:
            # Conversion vers le mod√®le Pydantic
            elements = []
            for i, suggestion in enumerate(analysis.get("suggestions", [])):
                elements.append(EbiosElement(
                    id=f"element_{i}",
                    name=f"√âl√©ment {i+1}",
                    description=suggestion,
                    category="suggestion",
                    criticality="medium",
                    confidence=0.8
                ))
            
            result = WorkshopAnalysisResult(
                mission_id=analysis.get("mission_id", "unknown"),
                completion_percentage=analysis.get("completion_percentage", 0),
                quality_score=analysis.get("quality_metrics", {}).get("completeness", 0),
                coherence_score=analysis.get("quality_metrics", {}).get("coherence", 0),
                ebios_compliance=analysis.get("quality_metrics", {}).get("ebios_compliance", 0),
                elements=elements,
                suggestions=analysis.get("suggestions", []),
                next_steps=[step for step in analysis.get("next_steps", []) if step]
            )
            
            logger.info("üìã R√©sultat structur√© avec Instructor")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur structuration Instructor: {e}")
            return self._create_basic_result(analysis, analysis.get("mission_id", "unknown"))
    
    def _create_basic_result(
        self, 
        analysis: Dict[str, Any], 
        mission_id: str
    ) -> WorkshopAnalysisResult:
        """Cr√©e un r√©sultat basique sans Instructor"""
        return WorkshopAnalysisResult(
            mission_id=mission_id,
            completion_percentage=analysis.get("completion_percentage", 0),
            quality_score=analysis.get("quality_metrics", {}).get("completeness", 0),
            coherence_score=analysis.get("quality_metrics", {}).get("coherence", 0),
            ebios_compliance=analysis.get("quality_metrics", {}).get("ebios_compliance", 0),
            elements=[],
            suggestions=analysis.get("suggestions", []),
            next_steps=[step for step in analysis.get("next_steps", []) if step]
        )
    
    def _create_fallback_result(
        self, 
        mission_id: str, 
        workshop_data: Dict[str, Any]
    ) -> WorkshopAnalysisResult:
        """Cr√©e un r√©sultat de fallback en cas d'erreur"""
        return WorkshopAnalysisResult(
            mission_id=mission_id,
            completion_percentage=25.0,
            quality_score=50.0,
            coherence_score=50.0,
            ebios_compliance=40.0,
            elements=[],
            suggestions=["Analyse de base disponible", "Services avanc√©s temporairement indisponibles"],
            next_steps=["V√©rifier la configuration", "R√©essayer l'analyse"]
        )
    
    async def _save_context(
        self, 
        mission_id: str, 
        result: WorkshopAnalysisResult,
        context: Dict[str, Any]
    ):
        """Sauvegarde le contexte pour la m√©moire persistante"""
        context_data = {
            "last_analysis": result.analysis_timestamp.isoformat(),
            "completion_percentage": result.completion_percentage,
            "quality_score": result.quality_score,
            "session_id": self.session_id,
            **context
        }
        
        # Sauvegarder dans Redis si disponible
        if self.redis_client:
            try:
                self.redis_client.setex(
                    f"context:{mission_id}",
                    3600,  # 1 heure d'expiration
                    json.dumps(context_data, default=str)
                )
                logger.info(f"üíæ Contexte sauvegard√© dans Redis: {mission_id}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur sauvegarde Redis: {e}")
        
        # Sauvegarder en m√©moire locale
        self.memory_store[mission_id] = context_data
        logger.info(f"üíæ Contexte sauvegard√© en m√©moire locale: {mission_id}")

    async def _analyze_with_semantic_ai(
        self,
        workshop_data: Dict[str, Any],
        existing_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyse avec l'IA s√©mantique avanc√©e"""
        try:
            semantic_analyzer = self.advanced_ai_services['semantic_analyzer']

            # Pr√©parer tous les √©l√©ments pour l'analyse s√©mantique
            all_elements = []
            for category in ['business_values', 'essential_assets', 'supporting_assets', 'dreaded_events']:
                for item in workshop_data.get(category, []):
                    all_elements.append({
                        'id': item.get('id', f"{category}_{len(all_elements)}"),
                        'name': item.get('name', ''),
                        'description': item.get('description', ''),
                        'category': category
                    })

            if all_elements:
                # Effectuer l'analyse s√©mantique compl√®te
                semantic_result = await semantic_analyzer.analyze_ebios_elements(
                    all_elements,
                    analysis_type="comprehensive"
                )

                # Int√©grer les r√©sultats dans l'analyse existante
                semantic_enhancement = {
                    "semantic_analysis": {
                        "coherence_score": semantic_result.coherence_score,
                        "clusters": semantic_result.clusters,
                        "inconsistencies": semantic_result.inconsistencies,
                        "semantic_suggestions": semantic_result.suggestions,
                        "semantic_graph": semantic_result.semantic_graph
                    }
                }

                # Enrichir les suggestions existantes (convertir en strings)
                semantic_suggestions_text = []
                for suggestion in semantic_result.suggestions:
                    if isinstance(suggestion, dict):
                        semantic_suggestions_text.append(suggestion.get('suggestion', str(suggestion)))
                    else:
                        semantic_suggestions_text.append(str(suggestion))

                if "suggestions" in existing_analysis:
                    existing_analysis["suggestions"].extend(semantic_suggestions_text)
                else:
                    existing_analysis["suggestions"] = semantic_suggestions_text

                # Am√©liorer le score de coh√©rence
                if "quality_metrics" in existing_analysis:
                    existing_analysis["quality_metrics"]["semantic_coherence"] = semantic_result.coherence_score

                logger.info(f"‚úÖ Analyse s√©mantique int√©gr√©e - Score: {semantic_result.coherence_score:.2f}")
                return semantic_enhancement

            return {}

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse s√©mantique: {e}")
            return {}

    async def _analyze_with_ml_engine(
        self,
        workshop_data: Dict[str, Any],
        context: Optional[Dict[str, Any]],
        existing_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyse avec le moteur ML avanc√©"""
        try:
            ml_engine = self.advanced_ai_services['ml_suggestion_engine']

            # Effectuer l'analyse ML
            ml_result = await ml_engine.generate_ml_suggestions(workshop_data, context)

            # Int√©grer les r√©sultats ML
            ml_enhancement = {
                "ml_analysis": {
                    "quality_predictions": ml_result.quality_predictions,
                    "completion_score": ml_result.completion_score,
                    "risk_assessment": ml_result.risk_assessment,
                    "feature_importance": ml_result.feature_importance,
                    "model_confidence": ml_result.model_confidence,
                    "ml_suggestions": [suggestion.__dict__ for suggestion in ml_result.suggestions]
                }
            }

            # Enrichir les suggestions existantes avec les suggestions ML
            if "suggestions" in existing_analysis:
                ml_suggestions_text = [suggestion.content for suggestion in ml_result.suggestions]
                existing_analysis["suggestions"].extend(ml_suggestions_text)
            else:
                existing_analysis["suggestions"] = [suggestion.content for suggestion in ml_result.suggestions]

            # Am√©liorer les m√©triques de qualit√© avec les pr√©dictions ML
            if "quality_metrics" in existing_analysis:
                if ml_result.quality_predictions:
                    existing_analysis["quality_metrics"].update({
                        "ml_quality_prediction": ml_result.quality_predictions.get("overall_quality", 0),
                        "ml_completion_score": ml_result.completion_score,
                        "ml_confidence": ml_result.model_confidence
                    })

            # Ajouter l'√©valuation des risques
            existing_analysis["risk_assessment"] = ml_result.risk_assessment

            logger.info(f"‚úÖ Analyse ML int√©gr√©e - Confiance: {ml_result.model_confidence:.2f}")
            return ml_enhancement

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse ML: {e}")
            return {}

    async def _analyze_with_rag_service(
        self,
        workshop_data: Dict[str, Any],
        context: Optional[Dict[str, Any]],
        existing_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyse avec le service RAG EBIOS RM"""
        try:
            rag_service = self.rag_services['rag_service']

            # Construire des requ√™tes contextuelles pour RAG
            queries = self._build_rag_queries(workshop_data, context)

            rag_responses = []
            total_confidence = 0.0

            for query in queries:
                # Interroger la base de connaissances RAG
                rag_result = await rag_service.query_ebios_knowledge(query, context)
                rag_responses.append(rag_result)
                total_confidence += rag_result.confidence

            # Calculer la confiance moyenne
            avg_confidence = total_confidence / len(queries) if queries else 0.0

            # Extraire les suggestions et conseils du RAG
            rag_suggestions = []
            rag_sources = []

            for response in rag_responses:
                if response.response and response.confidence > 0.3:  # Seuil de confiance
                    rag_suggestions.append(f"Conseil expert: {response.response[:200]}...")
                    rag_sources.extend(response.sources)

            # Int√©grer les r√©sultats RAG
            rag_enhancement = {
                "rag_analysis": {
                    "queries_processed": len(queries),
                    "average_confidence": avg_confidence,
                    "expert_suggestions": rag_suggestions,
                    "knowledge_sources": rag_sources,
                    "rag_responses": [
                        {
                            "query": resp.query,
                            "response": resp.response[:300] + "..." if len(resp.response) > 300 else resp.response,
                            "confidence": resp.confidence,
                            "sources_count": len(resp.sources)
                        }
                        for resp in rag_responses
                    ]
                }
            }

            # Enrichir les suggestions existantes avec les conseils RAG (s'assurer que ce sont des strings)
            rag_suggestions_text = [str(suggestion) for suggestion in rag_suggestions]

            if "suggestions" in existing_analysis:
                existing_analysis["suggestions"].extend(rag_suggestions_text)
            else:
                existing_analysis["suggestions"] = rag_suggestions_text

            # Ajouter les sources de connaissances
            existing_analysis["knowledge_sources"] = rag_sources

            logger.info(f"‚úÖ Analyse RAG int√©gr√©e - Confiance: {avg_confidence:.2f}")
            return rag_enhancement

        except Exception as e:
            logger.error(f"‚ùå Erreur analyse RAG: {e}")
            return {}

    def _build_rag_queries(
        self,
        workshop_data: Dict[str, Any],
        context: Optional[Dict[str, Any]]
    ) -> List[str]:
        """Construit des requ√™tes contextuelles pour RAG"""
        queries = []

        try:
            current_step = context.get('current_step', '') if context else ''

            # Requ√™tes g√©n√©rales EBIOS RM
            queries.append("Quelles sont les bonnes pratiques pour l'Atelier 1 EBIOS RM ?")

            # Requ√™tes sp√©cifiques selon l'√©tape
            if current_step == 'business-values' or len(workshop_data.get('business_values', [])) == 0:
                queries.append("Comment identifier et d√©finir les valeurs m√©tier en EBIOS RM ?")
                queries.append("Quels sont les exemples de valeurs m√©tier dans EBIOS RM ?")

            if current_step == 'essential-assets' or len(workshop_data.get('essential_assets', [])) == 0:
                queries.append("Comment identifier les biens essentiels en EBIOS RM ?")
                queries.append("Quelle est la diff√©rence entre valeurs m√©tier et biens essentiels ?")

            if current_step == 'supporting-assets' or len(workshop_data.get('supporting_assets', [])) == 0:
                queries.append("Comment identifier les biens supports en EBIOS RM ?")
                queries.append("Quels types de biens supports existent en EBIOS RM ?")

            if current_step == 'dreaded-events' or len(workshop_data.get('dreaded_events', [])) == 0:
                queries.append("Comment d√©finir les √©v√©nements redout√©s en EBIOS RM ?")
                queries.append("Quels sont les crit√®res de s√©curit√© pour les √©v√©nements redout√©s ?")

            # Requ√™tes sur la coh√©rence
            total_elements = sum([
                len(workshop_data.get('business_values', [])),
                len(workshop_data.get('essential_assets', [])),
                len(workshop_data.get('supporting_assets', [])),
                len(workshop_data.get('dreaded_events', []))
            ])

            if total_elements > 5:
                queries.append("Comment v√©rifier la coh√©rence entre les √©l√©ments EBIOS RM ?")
                queries.append("Quelles sont les erreurs courantes dans l'Atelier 1 EBIOS RM ?")

            return queries

        except Exception as e:
            logger.error(f"‚ùå Erreur construction requ√™tes RAG: {e}")
            return ["Bonnes pratiques EBIOS RM Atelier 1"]
    
    def is_ready(self) -> bool:
        """V√©rifie si l'orchestrateur est pr√™t"""
        return True
    
    def get_capabilities(self) -> Dict[str, bool]:
        """Retourne les capacit√©s disponibles"""
        capabilities = {
            "langchain_available": LANGCHAIN_AVAILABLE,
            "instructor_available": INSTRUCTOR_AVAILABLE,
            "redis_available": REDIS_AVAILABLE and self.redis_client is not None,
            "existing_services": EXISTING_SERVICES_AVAILABLE,
            "memory_persistent": self.redis_client is not None,
            "advanced_ai_services": ADVANCED_AI_SERVICES_AVAILABLE,
            "rag_services": RAG_SERVICES_AVAILABLE
        }

        # Ajouter les capacit√©s des services IA avanc√©s
        if ADVANCED_AI_SERVICES_AVAILABLE:
            if self.advanced_ai_services.get('semantic_analyzer'):
                semantic_caps = self.advanced_ai_services['semantic_analyzer'].get_capabilities()
                capabilities.update({f"semantic_{k}": v for k, v in semantic_caps.items()})

            if self.advanced_ai_services.get('ml_suggestion_engine'):
                ml_caps = self.advanced_ai_services['ml_suggestion_engine'].get_capabilities()
                capabilities.update({f"ml_{k}": v for k, v in ml_caps.items()})

        # Ajouter les capacit√©s des services RAG
        if RAG_SERVICES_AVAILABLE:
            if self.rag_services.get('rag_service'):
                rag_caps = self.rag_services['rag_service'].get_capabilities()
                capabilities.update({f"rag_{k}": v for k, v in rag_caps.items()})

            if self.rag_services.get('document_processor'):
                doc_caps = self.rag_services['document_processor'].get_capabilities()
                capabilities.update({f"doc_{k}": v for k, v in doc_caps.items()})

        return capabilities

# === FACTORY POUR CR√âATION S√âCURIS√âE ===

class Workshop1OrchestratorFactory:
    """Factory pour cr√©er l'orchestrateur de mani√®re s√©curis√©e"""
    
    @staticmethod
    def create() -> Workshop1Orchestrator:
        """Cr√©e un orchestrateur en mode s√©curis√©"""
        try:
            orchestrator = Workshop1Orchestrator()
            logger.info("‚úÖ Orchestrateur Workshop 1 cr√©√© avec succ√®s")
            return orchestrator
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation orchestrateur: {e}")
            # Retourner un orchestrateur minimal en cas d'erreur
            return Workshop1Orchestrator()

# Export principal
__all__ = ['Workshop1Orchestrator', 'Workshop1OrchestratorFactory', 'WorkshopAnalysisResult']
