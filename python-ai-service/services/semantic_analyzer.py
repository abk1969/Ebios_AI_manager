"""
üß† ANALYSEUR S√âMANTIQUE AVANC√â EBIOS RM
Analyse s√©mantique avec Transformers + Sentence-Transformers
Point 2.1 - Phase 2: IA S√©mantique et Suggestions
"""

import asyncio
import logging
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import json

# Imports conditionnels pour √©viter les erreurs
try:
    from sentence_transformers import SentenceTransformer
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logging.warning("üîß Transformers non disponible, mode simulation activ√©")

try:
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("üîß Scikit-learn non disponible, mode simulation activ√©")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    logging.warning("üîß NetworkX non disponible, mode simulation activ√©")

logger = logging.getLogger(__name__)

# === MOD√àLES DE DONN√âES ===

class SemanticAnalysisResult:
    """R√©sultat d'analyse s√©mantique"""
    def __init__(self):
        self.similarity_matrix = None
        self.clusters = []
        self.inconsistencies = []
        self.suggestions = []
        self.coherence_score = 0.0
        self.semantic_graph = None
        self.analysis_timestamp = datetime.now()

class EbiosElement:
    """√âl√©ment EBIOS RM pour analyse s√©mantique"""
    def __init__(self, id: str, name: str, description: str, category: str):
        self.id = id
        self.name = name
        self.description = description
        self.category = category
        self.embedding = None
        self.semantic_score = 0.0

# === ANALYSEUR S√âMANTIQUE PRINCIPAL ===

class EbiosSemanticAnalyzer:
    """
    Analyseur s√©mantique avanc√© pour EBIOS RM
    Utilise Sentence-Transformers pour l'analyse s√©mantique
    """
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.sentence_model = None
        self.embeddings_cache = {}
        self.analysis_cache = {}
        
        # Initialisation s√©curis√©e
        self._initialize_safely()
    
    def _initialize_safely(self):
        """Initialisation s√©curis√©e des mod√®les"""
        logger.info(f"üß† Initialisation Analyseur S√©mantique: {self.model_name}")
        
        if TRANSFORMERS_AVAILABLE:
            try:
                self.sentence_model = SentenceTransformer(self.model_name)
                logger.info("‚úÖ Mod√®le Sentence-Transformers charg√©")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur chargement mod√®le: {e}")
                self.sentence_model = None
        else:
            logger.warning("‚ö†Ô∏è Transformers non disponible, mode simulation")
    
    async def analyze_ebios_elements(
        self, 
        elements: List[Dict[str, Any]],
        analysis_type: str = "comprehensive"
    ) -> SemanticAnalysisResult:
        """
        Analyse s√©mantique compl√®te des √©l√©ments EBIOS RM
        """
        logger.info(f"üß† Analyse s√©mantique: {len(elements)} √©l√©ments, type: {analysis_type}")
        
        result = SemanticAnalysisResult()
        
        try:
            # 1. Conversion en objets EbiosElement
            ebios_elements = self._convert_to_ebios_elements(elements)
            
            # 2. G√©n√©ration des embeddings
            await self._generate_embeddings(ebios_elements)
            
            # 3. Analyse de similarit√©
            if analysis_type in ["comprehensive", "similarity"]:
                result.similarity_matrix = self._compute_similarity_matrix(ebios_elements)
            
            # 4. Clustering s√©mantique
            if analysis_type in ["comprehensive", "clustering"]:
                result.clusters = self._perform_semantic_clustering(ebios_elements)
            
            # 5. D√©tection d'incoh√©rences
            if analysis_type in ["comprehensive", "inconsistencies"]:
                result.inconsistencies = self._detect_semantic_inconsistencies(ebios_elements)
            
            # 6. G√©n√©ration de suggestions
            result.suggestions = self._generate_semantic_suggestions(ebios_elements, result)
            
            # 7. Score de coh√©rence global
            result.coherence_score = self._compute_coherence_score(ebios_elements, result)
            
            # 8. Graphe s√©mantique
            if NETWORKX_AVAILABLE and analysis_type == "comprehensive":
                result.semantic_graph = self._build_semantic_graph(ebios_elements, result)
            
            logger.info(f"‚úÖ Analyse s√©mantique termin√©e - Score: {result.coherence_score:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse s√©mantique: {e}")
            return self._create_fallback_result(elements)
    
    def _convert_to_ebios_elements(self, elements: List[Dict[str, Any]]) -> List[EbiosElement]:
        """Convertit les √©l√©ments en objets EbiosElement"""
        ebios_elements = []
        
        for element in elements:
            ebios_element = EbiosElement(
                id=element.get('id', f"elem_{len(ebios_elements)}"),
                name=element.get('name', ''),
                description=element.get('description', ''),
                category=element.get('category', 'unknown')
            )
            ebios_elements.append(ebios_element)
        
        return ebios_elements
    
    async def _generate_embeddings(self, elements: List[EbiosElement]):
        """G√©n√®re les embeddings pour chaque √©l√©ment"""
        if not self.sentence_model:
            # Mode simulation
            for element in elements:
                element.embedding = np.random.rand(384)  # Dimension du mod√®le MiniLM
            return
        
        try:
            # Pr√©parer les textes pour l'embedding
            texts = []
            for element in elements:
                # Combiner nom et description pour un embedding plus riche
                text = f"{element.name}. {element.description}".strip()
                texts.append(text)
            
            # G√©n√©rer les embeddings en batch
            embeddings = self.sentence_model.encode(texts, convert_to_numpy=True)
            
            # Assigner les embeddings aux √©l√©ments
            for i, element in enumerate(elements):
                element.embedding = embeddings[i]
                
                # Calculer un score s√©mantique basique
                element.semantic_score = float(np.linalg.norm(embeddings[i]))
            
            logger.info(f"‚úÖ Embeddings g√©n√©r√©s pour {len(elements)} √©l√©ments")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration embeddings: {e}")
            # Fallback avec embeddings al√©atoires
            for element in elements:
                element.embedding = np.random.rand(384)
    
    def _compute_similarity_matrix(self, elements: List[EbiosElement]) -> np.ndarray:
        """Calcule la matrice de similarit√© entre √©l√©ments"""
        if not SKLEARN_AVAILABLE or not elements:
            return np.eye(len(elements)) if elements else np.array([])
        
        try:
            # Extraire les embeddings
            embeddings = np.array([elem.embedding for elem in elements])
            
            # Calculer la similarit√© cosinus
            similarity_matrix = cosine_similarity(embeddings)
            
            logger.info(f"‚úÖ Matrice de similarit√© calcul√©e: {similarity_matrix.shape}")
            return similarity_matrix
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul similarit√©: {e}")
            return np.eye(len(elements))
    
    def _perform_semantic_clustering(self, elements: List[EbiosElement]) -> List[Dict[str, Any]]:
        """Effectue un clustering s√©mantique des √©l√©ments"""
        if not SKLEARN_AVAILABLE or len(elements) < 2:
            return []
        
        try:
            # Extraire les embeddings
            embeddings = np.array([elem.embedding for elem in elements])
            
            # D√©terminer le nombre optimal de clusters
            n_clusters = min(max(2, len(elements) // 3), 5)
            
            # Effectuer le clustering
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # Organiser les r√©sultats par cluster
            clusters = []
            for cluster_id in range(n_clusters):
                cluster_elements = [
                    elements[i] for i, label in enumerate(cluster_labels) 
                    if label == cluster_id
                ]
                
                if cluster_elements:
                    # Calculer le centro√Øde du cluster
                    cluster_embeddings = [elem.embedding for elem in cluster_elements]
                    centroid = np.mean(cluster_embeddings, axis=0)
                    
                    # Trouver l'√©l√©ment le plus repr√©sentatif
                    distances = [
                        np.linalg.norm(elem.embedding - centroid) 
                        for elem in cluster_elements
                    ]
                    representative_idx = np.argmin(distances)
                    
                    clusters.append({
                        "cluster_id": cluster_id,
                        "elements": [elem.id for elem in cluster_elements],
                        "representative": cluster_elements[representative_idx].id,
                        "theme": cluster_elements[representative_idx].name,
                        "coherence": float(1.0 - np.mean(distances))
                    })
            
            logger.info(f"‚úÖ Clustering effectu√©: {len(clusters)} clusters")
            return clusters
            
        except Exception as e:
            logger.error(f"‚ùå Erreur clustering: {e}")
            return []
    
    def _detect_semantic_inconsistencies(self, elements: List[EbiosElement]) -> List[Dict[str, Any]]:
        """D√©tecte les incoh√©rences s√©mantiques"""
        inconsistencies = []
        
        if not elements or len(elements) < 2:
            return inconsistencies
        
        try:
            # Calculer la matrice de similarit√©
            similarity_matrix = self._compute_similarity_matrix(elements)
            
            # D√©tecter les √©l√©ments avec des similarit√©s anormalement faibles
            for i, element_i in enumerate(elements):
                for j, element_j in enumerate(elements[i+1:], i+1):
                    similarity = similarity_matrix[i][j]
                    
                    # Si deux √©l√©ments de m√™me cat√©gorie ont une similarit√© tr√®s faible
                    if (element_i.category == element_j.category and 
                        similarity < 0.3):  # Seuil de similarit√©
                        
                        inconsistencies.append({
                            "type": "low_similarity_same_category",
                            "element1": element_i.id,
                            "element2": element_j.id,
                            "similarity": float(similarity),
                            "severity": "medium",
                            "description": f"Similarit√© faible entre √©l√©ments de m√™me cat√©gorie: {element_i.name} et {element_j.name}"
                        })
                    
                    # Si deux √©l√©ments de cat√©gories diff√©rentes ont une similarit√© tr√®s √©lev√©e
                    elif (element_i.category != element_j.category and 
                          similarity > 0.8):  # Seuil de similarit√© √©lev√©e
                        
                        inconsistencies.append({
                            "type": "high_similarity_different_category",
                            "element1": element_i.id,
                            "element2": element_j.id,
                            "similarity": float(similarity),
                            "severity": "low",
                            "description": f"Similarit√© √©lev√©e entre √©l√©ments de cat√©gories diff√©rentes: {element_i.name} et {element_j.name}"
                        })
            
            # D√©tecter les √©l√©ments isol√©s (faible similarit√© avec tous les autres)
            for i, element in enumerate(elements):
                avg_similarity = np.mean([
                    similarity_matrix[i][j] for j in range(len(elements)) if i != j
                ])
                
                if avg_similarity < 0.2:  # Seuil d'isolement
                    inconsistencies.append({
                        "type": "isolated_element",
                        "element": element.id,
                        "avg_similarity": float(avg_similarity),
                        "severity": "high",
                        "description": f"√âl√©ment isol√© s√©mantiquement: {element.name}"
                    })
            
            logger.info(f"‚úÖ D√©tection incoh√©rences: {len(inconsistencies)} trouv√©es")
            return inconsistencies
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection incoh√©rences: {e}")
            return []
    
    def _generate_semantic_suggestions(
        self, 
        elements: List[EbiosElement], 
        result: SemanticAnalysisResult
    ) -> List[Dict[str, Any]]:
        """G√©n√®re des suggestions d'am√©lioration s√©mantique"""
        suggestions = []
        
        try:
            # Suggestions bas√©es sur les clusters
            if result.clusters:
                for cluster in result.clusters:
                    if len(cluster["elements"]) == 1:
                        suggestions.append({
                            "type": "expand_cluster",
                            "priority": "medium",
                            "cluster_id": cluster["cluster_id"],
                            "suggestion": f"Consid√©rez d'ajouter des √©l√©ments li√©s au th√®me '{cluster['theme']}'",
                            "rationale": "Cluster avec un seul √©l√©ment d√©tect√©"
                        })
            
            # Suggestions bas√©es sur les incoh√©rences
            for inconsistency in result.inconsistencies:
                if inconsistency["type"] == "isolated_element":
                    suggestions.append({
                        "type": "clarify_element",
                        "priority": "high",
                        "element_id": inconsistency["element"],
                        "suggestion": "Clarifiez ou enrichissez la description de cet √©l√©ment",
                        "rationale": "√âl√©ment s√©mantiquement isol√© des autres"
                    })
                
                elif inconsistency["type"] == "low_similarity_same_category":
                    suggestions.append({
                        "type": "harmonize_category",
                        "priority": "medium",
                        "elements": [inconsistency["element1"], inconsistency["element2"]],
                        "suggestion": "Harmonisez le vocabulaire entre ces √©l√©ments de m√™me cat√©gorie",
                        "rationale": "Similarit√© s√©mantique faible dans la m√™me cat√©gorie"
                    })
            
            # Suggestions g√©n√©rales
            if len(elements) < 3:
                suggestions.append({
                    "type": "add_elements",
                    "priority": "low",
                    "suggestion": "Consid√©rez d'ajouter plus d'√©l√©ments pour une analyse plus compl√®te",
                    "rationale": "Nombre d'√©l√©ments insuffisant pour une analyse robuste"
                })
            
            logger.info(f"‚úÖ Suggestions g√©n√©r√©es: {len(suggestions)}")
            return suggestions
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration suggestions: {e}")
            return []
    
    def _compute_coherence_score(
        self, 
        elements: List[EbiosElement], 
        result: SemanticAnalysisResult
    ) -> float:
        """Calcule un score de coh√©rence s√©mantique global"""
        if not elements:
            return 0.0
        
        try:
            scores = []
            
            # Score bas√© sur la similarit√© moyenne
            if result.similarity_matrix is not None and result.similarity_matrix.size > 0:
                # Exclure la diagonale (similarit√© avec soi-m√™me)
                mask = ~np.eye(result.similarity_matrix.shape[0], dtype=bool)
                avg_similarity = np.mean(result.similarity_matrix[mask])
                scores.append(avg_similarity * 100)
            
            # Score bas√© sur les clusters
            if result.clusters:
                cluster_coherence = np.mean([cluster["coherence"] for cluster in result.clusters])
                scores.append(cluster_coherence * 100)
            
            # P√©nalit√© pour les incoh√©rences
            inconsistency_penalty = len(result.inconsistencies) * 5
            
            # Score final
            if scores:
                base_score = np.mean(scores)
                final_score = max(0, min(100, base_score - inconsistency_penalty))
            else:
                final_score = 50.0  # Score neutre par d√©faut
            
            return final_score
            
        except Exception as e:
            logger.error(f"‚ùå Erreur calcul score coh√©rence: {e}")
            return 50.0
    
    def _build_semantic_graph(
        self, 
        elements: List[EbiosElement], 
        result: SemanticAnalysisResult
    ) -> Optional[Dict[str, Any]]:
        """Construit un graphe s√©mantique des relations"""
        if not NETWORKX_AVAILABLE or not elements:
            return None
        
        try:
            G = nx.Graph()
            
            # Ajouter les n≈ìuds
            for element in elements:
                G.add_node(element.id, 
                          name=element.name,
                          category=element.category,
                          semantic_score=element.semantic_score)
            
            # Ajouter les ar√™tes bas√©es sur la similarit√©
            if result.similarity_matrix is not None:
                threshold = 0.5  # Seuil de similarit√© pour cr√©er une ar√™te
                
                for i, element_i in enumerate(elements):
                    for j, element_j in enumerate(elements[i+1:], i+1):
                        similarity = result.similarity_matrix[i][j]
                        
                        if similarity > threshold:
                            G.add_edge(element_i.id, element_j.id, 
                                     weight=float(similarity),
                                     similarity=float(similarity))
            
            # Calculer les m√©triques du graphe
            graph_metrics = {
                "nodes": G.number_of_nodes(),
                "edges": G.number_of_edges(),
                "density": nx.density(G),
                "connected_components": nx.number_connected_components(G)
            }
            
            # Convertir en format s√©rialisable
            graph_data = {
                "nodes": [
                    {
                        "id": node,
                        "name": G.nodes[node]["name"],
                        "category": G.nodes[node]["category"],
                        "semantic_score": G.nodes[node]["semantic_score"]
                    }
                    for node in G.nodes()
                ],
                "edges": [
                    {
                        "source": edge[0],
                        "target": edge[1],
                        "weight": G.edges[edge]["weight"],
                        "similarity": G.edges[edge]["similarity"]
                    }
                    for edge in G.edges()
                ],
                "metrics": graph_metrics
            }
            
            logger.info(f"‚úÖ Graphe s√©mantique construit: {graph_metrics}")
            return graph_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur construction graphe: {e}")
            return None
    
    def _create_fallback_result(self, elements: List[Dict[str, Any]]) -> SemanticAnalysisResult:
        """Cr√©e un r√©sultat de fallback en cas d'erreur"""
        result = SemanticAnalysisResult()
        result.coherence_score = 50.0
        result.suggestions = [
            {
                "type": "fallback",
                "priority": "low",
                "suggestion": "Analyse s√©mantique de base disponible",
                "rationale": "Services avanc√©s temporairement indisponibles"
            }
        ]
        return result
    
    def is_ready(self) -> bool:
        """V√©rifie si l'analyseur est pr√™t"""
        return True  # Toujours pr√™t gr√¢ce au fallback
    
    def get_capabilities(self) -> Dict[str, bool]:
        """Retourne les capacit√©s disponibles"""
        return {
            "transformers_available": TRANSFORMERS_AVAILABLE,
            "sklearn_available": SKLEARN_AVAILABLE,
            "networkx_available": NETWORKX_AVAILABLE,
            "model_loaded": self.sentence_model is not None,
            "semantic_analysis": True,
            "clustering": SKLEARN_AVAILABLE,
            "graph_analysis": NETWORKX_AVAILABLE
        }

# === FACTORY ===

class SemanticAnalyzerFactory:
    """Factory pour cr√©er l'analyseur s√©mantique"""
    
    @staticmethod
    def create(model_name: str = "all-MiniLM-L6-v2") -> EbiosSemanticAnalyzer:
        """Cr√©e l'analyseur s√©mantique de mani√®re s√©curis√©e"""
        try:
            analyzer = EbiosSemanticAnalyzer(model_name)
            logger.info("‚úÖ Analyseur s√©mantique cr√©√© avec succ√®s")
            return analyzer
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation analyseur s√©mantique: {e}")
            # Retourner un analyseur minimal
            return EbiosSemanticAnalyzer()

# Export principal
__all__ = ['EbiosSemanticAnalyzer', 'SemanticAnalyzerFactory', 'SemanticAnalysisResult', 'EbiosElement']
